# Optimizing_Vision_Transformers
Optimizing Vision Transformers

# ğŸ” Efficient Vision Transformers on CIFAR-10

This repository explores techniques to compress and accelerate Vision Transformers (ViT) using:

- ğŸ”§ Structured **Pruning**
- ğŸ§  **Knowledge Distillation (KD)**
- ğŸ¤– Compact ViT architectures (reduced depth, dim, and heads)

Our experiments demonstrate significant reductions in model size, FLOPs, and power usage â€” while preserving or even improving classification accuracy.

---

## ğŸ“ Contents

- `train_cifar10.py` â€” Baseline ViT training script on CIFAR-10
- `vitprune.py` â€” Structured pruning implementation with `channel_selection`
- `distill_pruned.py` â€” Knowledge distillation from baseline to pruned ViT
- `metrics.py` â€” Evaluation utilities for measuring latency, FLOPs, model size, etc.
- `README.md` â€” You're here!

---

## ğŸ“Š Performance Summary

### ğŸ§ª Metrics Comparison

| Model                                              | Accuracy | Latency (ms) | Speed (samples/sec) | Model Size (MB) | Memory (MB) | FLOPs (GFLOPs) | Parameters (M) | Power (W) |
|---------------------------------------------------|----------|--------------|----------------------|------------------|-------------|----------------|----------------|-----------|
| **Baseline ViT**                                  | 78.33%   | 2.17         | 461.39               | 39.20            | 1546.10     | 0.62           | 9.75           | 28.46     |
| **Pruned 20%**                                     | 71.50%   | 1.99         | 501.33               | 34.15            | 1546.28     | 0.54           | 8.50           | 26.83     |
| **Pruned 30%**                                     | 70.93%   | 1.95         | 511.57               | 31.49            | 1559.09     | 0.49           | 7.83           | 25.18     |
| **Pruned 40%**                                     | 69.25%   | 1.94         | 516.20               | 28.24            | 1625.50     | 0.44           | 7.02           | 22.68     |
| **Pruned 40% + KD Fine-tuned**                    | 80.67%   | 1.88         | 531.63               | 28.27            | 1627.34     | 0.44           | 7.02           | 18.94     |
| **KD: Reduced Dim + Depth + Heads (Tiny ViT)**    | 77.64%   | 1.32         | 755.03               | 6.75             | 1290.65     | 0.10           | 1.66           | 7.82      |

---

### ğŸ“‰ % Change Compared to Baseline

| Model                                              | Accuracy Î” | Latency Î” | Speed Î”         | Model Size Î” | Memory Î”    | FLOPs Î”     | Params Î”     | Power Î”     |
|---------------------------------------------------|------------|-----------|------------------|---------------|--------------|--------------|---------------|-------------|
| **Pruned 20%**                                     | ğŸ”½ -8.71%  | ğŸ”½ -8.29% | ğŸ”¼ +8.66%        | ğŸ”½ -12.88%    | ğŸ”¼ +0.01%    | ğŸ”½ -12.90%   | ğŸ”½ -12.82%    | ğŸ”½ -5.73%    |
| **Pruned 30%**                                     | ğŸ”½ -9.44%  | ğŸ”½ -10.14%| ğŸ”¼ +10.88%       | ğŸ”½ -19.67%    | ğŸ”¼ +0.84%    | ğŸ”½ -20.97%   | ğŸ”½ -19.69%    | ğŸ”½ -11.52%   |
| **Pruned 40%**                                     | ğŸ”½ -11.61% | ğŸ”½ -10.60%| ğŸ”¼ +11.88%       | ğŸ”½ -27.96%    | ğŸ”¼ +5.14%    | ğŸ”½ -29.03%   | ğŸ”½ -27.90%    | ğŸ”½ -20.32%   |
| **Pruned 40% + KD Fine-tuned**                    | ğŸ”¼ +2.99%  | ğŸ”½ -13.36%| ğŸ”¼ +15.23%       | ğŸ”½ -27.89%    | ğŸ”¼ +5.25%    | ğŸ”½ -29.03%   | ğŸ”½ -27.90%    | ğŸ”½ -33.45%   |
| **KD: Reduced Dim + Depth + Heads**    | ğŸ”½ -0.88%  | ğŸ”½ -39.17%| ğŸ”¼ +63.68%       | ğŸ”½ -82.78%    | ğŸ”½ -16.51%   | ğŸ”½ -83.87%   | ğŸ”½ -82.97%    | ğŸ”½ -72.51%   |

---

## ğŸ§  Key Takeaways

- âœ… **Knowledge Distillation** can recover or improve performance after pruning.
- ğŸ’¡ **Reduced dim ViT with KD** deliver excellent speed/efficiency for edge deployment.
- ğŸ” Our pruning method achieves up to **28% model size reduction** with minimal accuracy loss.
- ğŸ§ª Detailed evaluation includes accuracy, latency, throughput, FLOPs, memory, and power estimation.

---